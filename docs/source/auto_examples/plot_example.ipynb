{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Calculate stopping criteria\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introduction\n\nWhen we do machine learning-prioritised screening in a systematic review, we\ncan only save work if we have a method for deciding when to stop.\nSeveral methods have been suggested, but those which rely on rules of thumb\n(like stopping after N consecutive irrelevant records) are not supported by either\ntheory or empirical evaluations. A particular value of N may work well in one\nreview, but do badly in another. The right value depends on the size of the dataset,\nthe prevalence of relevant documents, the effectiveness of the machine learning\nalgorithm, and a bit of luck. Moreover, using such a criterion does not allow\nus to say anything about our expected recall, nor our confidence in achieving it.\nIn callaghan_statistical_2020 we offered a theoretically well motivated stopping\ncriteria, which we demonstrated was safe to use. It allows you to communicate\nyour confidence in achieving any arbitrary recall target. This package aims to\nmake this stopping criteria easy to use for R users.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data\n\nLets initialise some test data. For demonstration purposes, we define the number of documents,\nand the prevalence of relevant documents. Then we simulate a prioritised screening-like\nprocess, where we sample documents, where we are `bias` times more likely to select a random\nrelevant document than a random irrelevant document.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nn_docs = 6000\nn_relevant_docs = 1000\nurn_bias = 20\n\nsample = np.zeros(n_docs)\nsample[:n_relevant_docs] = 1\n\nps = np.ones(n_docs)\nps[:n_relevant_docs] = urn_bias\nps = ps/ps.sum()\n\nsample = np.random.choice(sample,sample.shape,replace=False, p=ps)\ndf = pd.DataFrame({'relevance': sample})\ndf['id'] = df.index\ndf.relevance.cumsum().plot()\ndf.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## When is it safe to stop?\n\nLet's imagine we've seen just the first 20,000 documents. We can use our stopping criteria\nto calculate a p score for a null hypothesis that we have missed so many documents that we have\nnot achieved our recall target.\n\nIf the p score is low, then we can reject that null hypothesis and stop safely.\nThe lower the score, the more confident we can be about doing this. The p score\nis given by calculating the probability of observing the previous sequence\nof relevant and irrelevant documents, if there were enough remaining relevant\ndocuments to mean that our recall target had not been achieved.\n\nFor example, if we have seen 95 relevant documents,\nand our recall target is 95%, then there would have to be at least 6 relevant documents\nremaining for us to have missed our target. If we have just observed a sequence of 100 irrelevant\ndocument in a row, we ask how likely it would be to observe that by random sampling,\nif there were 6 relevant documents remaining.\n\nWe can calculate this using the buscarR package, by passing dataframe with a column\n`relevant` that contains 1s and 0s for relevant and irrelevant documents (and NAs for\ndocuments we have not seen yet). A separate column `seen` tells us if this document\nhas been seen by a human yet or not. The dataframe should have as many rows as there\nare unique documents in the dataset, should contain all that have been seen by a human\nand all documents that have not yet been seen by a human. The human-screened documents\nshould be in the order in which they were screened.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from buscarpy import calculate_h0\ndocuments_seen = 1500\ndf['seen_relevance'] = np.NaN\ndf.loc[:documents_seen,'seen_relevance'] = df.loc[:documents_seen,'relevance']\nfig, ax = plt.subplots()\ndf.seen_relevance.cumsum().plot()\nax.set_xlim(xmax=df.shape[0])\n\ncalculate_h0(df)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}