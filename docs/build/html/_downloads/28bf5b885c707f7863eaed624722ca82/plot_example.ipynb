{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Calculate stopping criteria\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introduction\n\nWhen we do machine learning-prioritised screening in a systematic review, we\ncan only save work if we have a method for deciding when to stop.\nSeveral methods have been suggested, but those which rely on rules of thumb\n(like stopping after N consecutive irrelevant records) are not supported by either\ntheory or empirical evaluations. A particular value of N may work well in one\nreview, but do badly in another. The right value depends on the size of the dataset,\nthe prevalence of relevant documents, the effectiveness of the machine learning\nalgorithm, and a bit of luck. Moreover, using such a criterion does not allow\nus to say anything about our expected recall, nor our confidence in achieving it.\nIn callaghan_statistical_2020 we offered a theoretically well motivated stopping\ncriteria, which we demonstrated was safe to use. It allows you to communicate\nyour confidence in achieving any arbitrary recall target. This package aims to\nmake this stopping criteria easy to use for R users.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data\n\nLets initialise some test data. The `generate_data` function takes a number\nof documents, as well as the  the prevalence of relevant documents, or uses\ndefault values, and simulates a prioritised screening-like\nprocess, where we sample documents, where we are `bias` times more likely to select a random\nrelevant document than a random irrelevant document.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom buscarpy import generate_dataset\n\ndf = generate_dataset(random_seed=12345)\ndf.relevant.cumsum().plot()\ndf.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## When is it safe to stop?\n\nLet's imagine we've seen just the first 10,000 documents. We can use our stopping criteria\nto calculate a p score for a null hypothesis that we have missed so many documents that we have\nnot achieved our recall target.\n\nIf the p score is low, then we can reject that null hypothesis and stop safely.\nThe lower the score, the more confident we can be about doing this. The p score\nis given by calculating the probability of observing the previous sequence\nof relevant and irrelevant documents, if there were enough remaining relevant\ndocuments to mean that our recall target had not been achieved.\n\nFor example, if we have seen 95 relevant documents,\nand our recall target is 95%, then there would have to be at least 6 relevant documents\nremaining for us to have missed our target. If we have just observed a sequence of 100 irrelevant\ndocument in a row, we ask how likely it would be to observe that by random sampling,\nif there were 6 relevant documents remaining.\n\nWe can calculate this using the buscarR package, by passing a list of 1s and 0s\nto our `calculate_h0` function, along with the total number of documents in\nin the dataset. The list of 1s and 0s represents INCLUDE and EXCLUDE decisions\nby human screeners, and should be in the order in which the documents were\nscreened\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from buscarpy import calculate_h0\nseen_documents = df['relevant'][:10000]\n\nfig, ax = plt.subplots()\nseen_documents.cumsum().plot()\nax.set_xlim(xmax=df.shape[0])\n\ncalculate_h0(seen_documents, df.shape[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our p score indicates that we are not yet confident enough to stop screening.\nIf we \"see\" an additional 2,000 documents, this will change\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "seen_documents = df['relevant'][:12000]\n\nfig, ax = plt.subplots()\nseen_documents.cumsum().plot()\nax.set_xlim(xmax=df.shape[0])\n\ncalculate_h0(seen_documents, df.shape[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now be **very confident** that we have **not missed** our recall target\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Changing recall targets\n\nWe can calculate the same stopping criteria for a different **recall target**,\nsimply by using the `recall_target` argument in `calculate_h0`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "calculate_h0(seen_documents, df.shape[0], recall_target=0.99)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we increase the recall target, we become **less** confident that we\nhave **not missed** our target.\n\nIn many practical cases, we may not be very confident in one target, but much\nmore confident in a target that is only a little smaller. The `recall_frontier`\nfunction calculates and plots the p score for several different recall targets,\nhelping to inform and transparently communicate our decision about the safety\nof stopping screening at any given point.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from buscarpy import recall_frontier\n\nrecall_frontier(seen_documents, df.shape[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Retrospective stopping criteria\n\nThe package also includes a helper function to calculate the stopping criteria\nat each point on a curve that has already been seen. By default we calculate\nthis after each batch of 1,000 documents. Change the `batch_size` to alter this,\nthough be warned that reducing it will increase the number of calculations that\nneeds to be made.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from buscarpy import retrospective_h0\n\nretrospective_h0(seen_documents, df.shape[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Biased urns\n\nThe stopping criteria, which is described in\n[Callaghan and M\u00fcller-Hansen, 2020](https://doi.org/10.1186/s13643-020-01521-4)\nassumes that the documents we have screened previously were drawn *at random*\nfrom the remaining records. This assumption is *conservative*, as the\nmachine-learning process should make it more likely that we pick a relevant\ndocument than an irrelevant document.\n\nBeing conservative, it is safe to use this stopping criteria (and evaluations\nshow that it is wrong less than 5% of the time if the confidence level is set\nto 95%), but its conservative nature means that we will stop later than we\nstrictly need to.\n\nBiased urn theory offers us a more realistic set of assumptions, as it\ndescribes the probability distribution given a situation where we are more\nlikely to select one type of item than another. We can implement this in\nbuscarpy, by setting the `bias` parameter of our functions. `bias` describes\nhow much more likely it is to select a relevant than a non-relevant document.\n\nHowever, estimating this parameter is non-trivial, and work\non how to do this safely is currently ongoing.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "h0_1 = retrospective_h0(seen_documents, df.shape[0], bias=1, plot=False)\nh0_5 = retrospective_h0(seen_documents, df.shape[0], bias=5, plot=False)\n\nfig, ax = plt.subplots()\n\nax.plot(seen_documents.cumsum())\nax2 = ax.twinx()\nax2.scatter(h0_1['batch_sizes'], h0_1['p'], label='Unbiased')\nax2.scatter(h0_5['batch_sizes'], h0_5['p'], label='Bias==5')\nax.set_xlabel('Documents screened')\nax.set_ylabel('Relevant documents identified')\nax2.set_ylabel('p score for H0 that recall target missed')\n\nax2.legend()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}